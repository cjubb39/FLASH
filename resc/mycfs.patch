diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index 22b0f1e..0472bbf 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -17,7 +17,7 @@ CFLAGS_REMOVE_return_address.o = -pg
 
 obj-y		:= elf.o entry-armv.o entry-common.o irq.o opcodes.o \
 		   process.o ptrace.o return_address.o sched_clock.o \
-		   setup.o signal.o stacktrace.o sys_arm.o time.o traps.o
+		   setup.o signal.o stacktrace.o sys_arm.o time.o traps.o hw4.o
 
 obj-$(CONFIG_DEPRECATED_PARAM_STRUCT) += compat.o
 
diff --git a/arch/arm/kernel/calls.S b/arch/arm/kernel/calls.S
index 463ff4a..5e95e86 100644
--- a/arch/arm/kernel/calls.S
+++ b/arch/arm/kernel/calls.S
@@ -387,6 +387,7 @@
 /* 375 */	CALL(sys_setns)
 		CALL(sys_process_vm_readv)
 		CALL(sys_process_vm_writev)
+		CALL(sys_sched_setlimit)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
 #define syscalls_counted
diff --git a/arch/arm/kernel/hw4.c b/arch/arm/kernel/hw4.c
new file mode 100644
index 0000000..86f3027
--- /dev/null
+++ b/arch/arm/kernel/hw4.c
@@ -0,0 +1,13 @@
+#include <linux/sched.h>
+
+asmlinkage int sys_sched_setlimit(pid_t pid, unsigned long sched_limit)
+{
+	struct task_struct *t = find_task_by_vpid(pid);
+
+	if (!t)
+		return -1;
+
+	t->sme.sched_limit = sched_limit;
+
+	return 0;
+}
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff6bb0f..d742f2c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_MYCFS		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1232,6 +1233,20 @@ struct sched_entity {
 #endif
 };
 
+struct sched_mycfs_entity {
+	struct rb_node		run_node;
+	unsigned int		on_rq;
+
+	u64			exec_start;
+	u64			vruntime;
+
+	unsigned long		sched_limit;
+	u64			period_start;
+	u64			period_vruntime;
+	struct list_head	limited_list;
+	struct timer_list	period_timer;
+};
+
 struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned long timeout;
@@ -1280,6 +1295,7 @@ struct task_struct {
 	unsigned int rt_priority;
 	const struct sched_class *sched_class;
 	struct sched_entity se;
+	struct sched_mycfs_entity sme;
 	struct sched_rt_entity rt;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 3ede7d9..ebac653 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o sched_avg.o
+obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o sched_avg.o mycfs.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1cee48f..e4a3ca8 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1731,6 +1731,14 @@ static void __sched_fork(struct task_struct *p)
 	p->se.vruntime			= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
+	p->sme.on_rq			= 0;
+	p->sme.exec_start		= 0;
+	p->sme.vruntime			= 0;
+	p->sme.period_start		= 0;
+	p->sme.period_vruntime		= 0;
+	INIT_LIST_HEAD(&p->sme.limited_list);
+	init_timer(&p->sme.period_timer);
+
 #ifdef CONFIG_SCHEDSTATS
 	memset(&p->se.statistics, 0, sizeof(p->se.statistics));
 #endif
@@ -1784,8 +1792,12 @@ void sched_fork(struct task_struct *p)
 		p->sched_reset_on_fork = 0;
 	}
 
-	if (!rt_prio(p->prio))
-		p->sched_class = &fair_sched_class;
+	if (!rt_prio(p->prio)) {
+		if (p->policy == SCHED_MYCFS)
+			p->sched_class = &mycfs_sched_class;
+		else
+			p->sched_class = &fair_sched_class;
+	}
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -3869,6 +3881,8 @@ void rt_mutex_setprio(struct task_struct *p, int prio)
 
 	if (rt_prio(prio))
 		p->sched_class = &rt_sched_class;
+	else if (p->policy == SCHED_MYCFS)
+		p->sched_class = &mycfs_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 
@@ -4061,6 +4075,8 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 	p->prio = rt_mutex_getprio(p);
 	if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	else if (policy == SCHED_MYCFS)
+		p->sched_class = &mycfs_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 	set_load_weight(p);
@@ -4107,7 +4123,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_MYCFS && policy != SCHED_IDLE)
 			return -EINVAL;
 	}
 
@@ -4120,7 +4136,8 @@ recheck:
 	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
 	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
 		return -EINVAL;
-	if (rt_policy(policy) != (param->sched_priority != 0))
+	if (rt_policy(policy) != (param->sched_priority != 0) &&
+	    policy != SCHED_MYCFS)
 		return -EINVAL;
 
 	/*
@@ -7002,6 +7019,7 @@ void __init sched_init(void)
 		rq->calc_load_active = 0;
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
+		init_mycfs_rq(&rq->mycfs);
 		init_rt_rq(&rq->rt, rq);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
@@ -7712,7 +7730,7 @@ static int cpu_cgroup_can_attach(struct cgroup *cgrp,
 			return -EINVAL;
 #else
 		/* We don't support RT-tasks being in separate groups */
-		if (task->sched_class != &fair_sched_class)
+		if (task->sched_class != &fair_sched_class && task->sched_class != &mycfs_sched_class)
 			return -EINVAL;
 #endif
 	}
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index fc60d5b..d240ab2 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5536,7 +5536,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &mycfs_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/mycfs.c b/kernel/sched/mycfs.c
new file mode 100644
index 0000000..be18998
--- /dev/null
+++ b/kernel/sched/mycfs.c
@@ -0,0 +1,424 @@
+#include <linux/sched.h>
+
+#include "sched.h"
+
+static unsigned int sched_latency = 10000000ULL;
+extern unsigned int sysctl_sched_wakeup_granularity;
+
+static u64 period_length = 100000000ULL;
+
+const struct sched_class mycfs_sched_class;
+
+static inline struct task_struct *task_of(struct sched_mycfs_entity *sme)
+{
+	return container_of(sme, struct task_struct, sme);
+}
+
+static inline struct rq *rq_of(struct mycfs_rq *mycfs_rq)
+{
+	return container_of(mycfs_rq, struct rq, mycfs);
+}
+
+static inline struct mycfs_rq *task_mycfs_rq(struct task_struct *p)
+{
+	return &task_rq(p)->mycfs;
+}
+
+static inline struct mycfs_rq *mycfs_rq_of(struct sched_mycfs_entity *sme)
+{
+	struct task_struct *p = task_of(sme);
+	struct rq *rq = task_rq(p);
+
+	return &rq->mycfs;
+}
+
+static inline u64 max_vruntime(u64 min_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - min_vruntime);
+	if (delta > 0)
+		min_vruntime = vruntime;
+
+	return min_vruntime;
+}
+
+static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - min_vruntime);
+	if (delta < 0)
+		min_vruntime = vruntime;
+
+	return min_vruntime;
+}
+
+static void update_min_vruntime(struct mycfs_rq *mycfs_rq)
+{
+	u64 vruntime = mycfs_rq->min_vruntime;
+
+	if (mycfs_rq->curr)
+		vruntime = mycfs_rq->curr->vruntime;
+
+	if (mycfs_rq->rb_leftmost) {
+		struct sched_mycfs_entity *sme = rb_entry(mycfs_rq->rb_leftmost, struct sched_mycfs_entity, run_node);
+
+		if (!mycfs_rq->curr)
+			vruntime = sme->vruntime;
+		else
+			vruntime = min_vruntime(vruntime, sme->vruntime);
+	}
+
+	mycfs_rq->min_vruntime = max_vruntime(mycfs_rq->min_vruntime, vruntime);
+}
+
+static void __enqueue_entity(struct mycfs_rq *mycfs_rq, struct sched_mycfs_entity *sme)
+{
+	struct rb_node **link = &mycfs_rq->tasks_timeline.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_mycfs_entity *entry;
+	int leftmost = 1;
+
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_mycfs_entity, run_node);
+		if ((s64)(sme->vruntime - entry->vruntime) < 0) {
+			link = &parent->rb_left;
+		} else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		mycfs_rq->rb_leftmost = &sme->run_node;
+
+	rb_link_node(&sme->run_node, parent, link);
+	rb_insert_color(&sme->run_node, &mycfs_rq->tasks_timeline);
+}
+
+static void __dequeue_entity(struct mycfs_rq *mycfs_rq, struct sched_mycfs_entity *sme)
+{
+	if (mycfs_rq->rb_leftmost == &sme->run_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&sme->run_node);
+		mycfs_rq->rb_leftmost = next_node;
+	}
+
+	rb_erase(&sme->run_node, &mycfs_rq->tasks_timeline);
+}
+
+static void update_curr(struct mycfs_rq *mycfs_rq)
+{
+	struct sched_mycfs_entity *curr = mycfs_rq->curr;
+	u64 now = rq_of(mycfs_rq)->clock_task;
+	unsigned long delta_exec;
+
+	if (unlikely(!curr))
+		return;
+
+	delta_exec = (unsigned long)(now - curr->exec_start);
+	if (!delta_exec)
+		return;
+
+	curr->vruntime += delta_exec;
+	update_min_vruntime(mycfs_rq);
+	curr->exec_start = now;
+
+	if (now > curr->period_start + period_length + sched_latency) {
+		curr->period_start = now;
+		curr->period_vruntime = curr->vruntime;
+	}
+}
+
+static void
+set_next_entity(struct mycfs_rq *mycfs_rq, struct sched_mycfs_entity *sme)
+{
+	if (sme->on_rq)
+		__dequeue_entity(mycfs_rq, sme);
+
+	sme->exec_start = rq_of(mycfs_rq)->clock_task;
+	mycfs_rq->curr = sme;
+}
+
+static void
+enqueue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_mycfs_entity *sme = &p->sme;
+
+	if (!sme->on_rq) {
+		mycfs_rq = mycfs_rq_of(sme);
+
+		update_curr(mycfs_rq);
+		mycfs_rq->nr_running++;
+
+		sme->vruntime = max_vruntime(sme->vruntime, mycfs_rq->min_vruntime);
+
+		if (sme != mycfs_rq->curr)
+			__enqueue_entity(mycfs_rq, sme);
+		sme->on_rq = 1;
+	}
+
+	inc_nr_running(rq);
+}
+
+static void dequeue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_mycfs_entity *sme = &p->sme;
+
+	mycfs_rq = mycfs_rq_of(sme);
+
+	update_curr(mycfs_rq);
+
+	if (sme != mycfs_rq->curr)
+		__dequeue_entity(mycfs_rq, sme);
+	sme->on_rq = 0;
+	mycfs_rq->nr_running--;
+
+	update_min_vruntime(mycfs_rq);
+
+	dec_nr_running(rq);
+}
+
+#ifdef CONFIG_SMP
+static int
+select_task_rq_mycfs(struct task_struct *p, int sd_flag, int wake_flags)
+{
+	return task_cpu(p);
+}
+#endif /* CONFIG_SMP */
+
+static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_mycfs_entity *sme = &curr->sme, *psme = &p->sme;
+
+	if (unlikely(sme == psme))
+		return;
+
+	if (test_tsk_need_resched(curr))
+		return;
+
+	if (unlikely(p->policy != SCHED_MYCFS))
+		return;
+
+	update_curr(mycfs_rq_of(sme));
+	if ((s64)(sme->vruntime - psme->vruntime) > sysctl_sched_wakeup_granularity)
+		goto preempt;
+
+	return;
+
+preempt:
+	resched_task(curr);
+}
+
+static struct task_struct *pick_next_task_mycfs(struct rq *rq)
+{
+	struct mycfs_rq *mycfs_rq = &rq->mycfs;
+	struct sched_mycfs_entity *sme, *tmp;
+	u64 now;
+
+	update_rq_clock(rq);
+	now = rq->clock_task;
+	list_for_each_entry_safe(sme, tmp, &mycfs_rq->limited_entities, limited_list) {
+		if (!sme->sched_limit || now > sme->period_start + period_length + sched_latency) {
+			list_del(&sme->limited_list);
+			__enqueue_entity(mycfs_rq, sme);
+			mycfs_rq->nr_running++;
+		}
+	}
+
+	if (!mycfs_rq->nr_running)
+		return NULL;
+
+	sme = rb_entry(mycfs_rq->rb_leftmost, struct sched_mycfs_entity, run_node);
+	set_next_entity(mycfs_rq, sme);
+
+	return task_of(sme);
+}
+
+static void __period_function(unsigned long data)
+{
+}
+
+static void put_prev_task_mycfs(struct rq *rq, struct task_struct *prev)
+{
+	struct sched_mycfs_entity *sme = &prev->sme;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(sme);
+
+	if (sme->on_rq) {
+		u64 now = rq->clock_task;
+
+		update_curr(mycfs_rq);
+
+		if (!sme->sched_limit || now > sme->period_start + period_length + sched_latency || (sme->vruntime - sme->period_vruntime) * 100 < period_length * sme->sched_limit)
+			__enqueue_entity(mycfs_rq, sme);
+		else {
+			list_add_tail(&sme->limited_list, &mycfs_rq->limited_entities);
+			mycfs_rq->nr_running--;
+
+			if (!timer_pending(&sme->period_timer)) {
+				init_timer(&sme->period_timer);
+				sme->period_timer.expires = jiffies + 10;
+				sme->period_timer.function = __period_function;
+				add_timer(&sme->period_timer);
+			}
+		}
+	}
+	mycfs_rq->curr = NULL;
+}
+
+static void yield_task_mycfs(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct mycfs_rq *mycfs_rq = task_mycfs_rq(curr);
+
+	if (unlikely(rq->nr_running == 1))
+		return;
+
+	update_rq_clock(rq);
+	update_curr(mycfs_rq);
+	rq->skip_clock_update = 1;
+}
+
+static bool yield_to_task_mycfs(struct rq *rq, struct task_struct *p, bool preempt)
+{
+	struct sched_mycfs_entity *sme = &p->sme;
+
+	if (!sme->on_rq)
+		return false;
+
+	yield_task_mycfs(rq);
+
+	return true;
+}
+
+static void task_tick_mycfs(struct rq *rq, struct task_struct *curr, int queued)
+{
+	struct sched_mycfs_entity *sme = &curr->sme;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(sme);
+
+	update_curr(mycfs_rq);
+
+	if (sme->sched_limit && (sme->vruntime - sme->period_vruntime) * 100 >= period_length * sme->sched_limit)
+		resched_task(rq->curr);
+	else if (mycfs_rq->nr_running > 1) {
+		struct sched_mycfs_entity *first = rb_entry(mycfs_rq->rb_leftmost, struct sched_mycfs_entity, run_node);
+
+		if ((s64)(sme->vruntime - first->vruntime) > sched_latency / (mycfs_rq->nr_running + !sme->on_rq))
+			resched_task(rq->curr);
+	}
+}
+
+static void task_fork_mycfs(struct task_struct *p)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_mycfs_entity *sme = &p->sme, *curr;
+	int this_cpu = smp_processor_id();
+	struct rq *rq = this_rq();
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+
+	update_rq_clock(rq);
+
+	mycfs_rq = task_mycfs_rq(current);
+	curr = mycfs_rq->curr;
+
+	if (unlikely(task_cpu(p) != this_cpu)) {
+		rcu_read_lock();
+		__set_task_cpu(p, this_cpu);
+		rcu_read_unlock();
+	}
+
+	update_curr(mycfs_rq);
+
+	if (curr)
+		sme->vruntime = curr->vruntime;
+	sme->vruntime = max_vruntime(sme->vruntime, mycfs_rq->min_vruntime);
+
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+static void
+prio_changed_mycfs(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	if (!p->sme.on_rq)
+		return;
+
+	if (rq->curr == p) {
+		if (p->prio > oldprio)
+			resched_task(rq->curr);
+	} else
+		check_preempt_curr(rq, p, 0);
+}
+
+static void switched_from_mycfs(struct rq *rq, struct task_struct *p)
+{
+	struct sched_mycfs_entity *sme = &p->sme;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(sme);
+
+	if (!sme->on_rq && p->state != TASK_RUNNING) {
+		sme->vruntime = max_vruntime(sme->vruntime, mycfs_rq->min_vruntime);
+	}
+}
+
+static void switched_to_mycfs(struct rq *rq, struct task_struct *p)
+{
+	if (!p->sme.on_rq)
+		return;
+
+	p->sme.sched_limit = 0;
+
+	if (rq->curr == p)
+		resched_task(rq->curr);
+	else
+		check_preempt_curr(rq, p, 0);
+}
+
+static void set_curr_task_mycfs(struct rq *rq)
+{
+	struct sched_mycfs_entity *sme = &rq->curr->sme;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(sme);
+
+	set_next_entity(mycfs_rq, sme);
+}
+
+void init_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+	mycfs_rq->tasks_timeline = RB_ROOT;
+	mycfs_rq->min_vruntime = (u64)(-(1LL << 20));
+	INIT_LIST_HEAD(&mycfs_rq->limited_entities);
+}
+
+static unsigned int get_rr_interval_mycfs(struct rq *rq, struct task_struct *task)
+{
+	return NS_TO_JIFFIES(sched_latency);
+}
+
+const struct sched_class mycfs_sched_class = {
+	.next			= &idle_sched_class,
+	.enqueue_task		= enqueue_task_mycfs,
+	.dequeue_task		= dequeue_task_mycfs,
+	.yield_task		= yield_task_mycfs,
+	.yield_to_task		= yield_to_task_mycfs,
+
+	.check_preempt_curr	= check_preempt_wakeup,
+
+	.pick_next_task		= pick_next_task_mycfs,
+	.put_prev_task		= put_prev_task_mycfs,
+
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_mycfs,
+#endif
+
+	.set_curr_task          = set_curr_task_mycfs,
+	.task_tick		= task_tick_mycfs,
+	.task_fork		= task_fork_mycfs,
+
+	.prio_changed		= prio_changed_mycfs,
+	.switched_from		= switched_from_mycfs,
+	.switched_to		= switched_to_mycfs,
+
+	.get_rr_interval	= get_rr_interval_mycfs,
+};
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5370bcb..bffa786 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -273,6 +273,19 @@ struct cfs_rq {
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
 
+struct mycfs_rq {
+	unsigned long nr_running;
+
+	u64 min_vruntime;
+
+	struct rb_root tasks_timeline;
+	struct rb_node *rb_leftmost;
+
+	struct sched_mycfs_entity *curr;
+
+	struct list_head limited_entities;
+};
+
 static inline int rt_bandwidth_enabled(void)
 {
 	return sysctl_sched_rt_runtime >= 0;
@@ -371,6 +384,7 @@ struct rq {
 	u64 nr_switches;
 
 	struct cfs_rq cfs;
+	struct mycfs_rq mycfs;
 	struct rt_rq rt;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -855,6 +869,7 @@ enum cpuacct_stat_index {
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
+extern const struct sched_class mycfs_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
 
@@ -1154,6 +1169,7 @@ extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
+extern void init_mycfs_rq(struct mycfs_rq *mycfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
