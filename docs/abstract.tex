\documentclass{sig-alternate-10pt}

\newcommand{\ttt}{\texttt}

\usepackage{graphicx}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{booktabs}

\title{FLASH\\Fast Linux Advanced Scheduler Hardware}
\author{
	Mark Aligbe \\
	    \email{ma2799@columbia.edu}
	\and
    Chae Jubb \\
        \email{ecj2122@columbia.edu}
}
\date{7 April 2015}

\begin{document}
\maketitle

\begin{abstract}
% \lipsum[1]

As accelerators become more common and necessary as a way to continue Moore's Law in the absence of Dennard Scaling, researchers explore segments of computation that can be improved with the aid of dedicated hardware. This paper presents FLASH, a hardware scheduler to take over the task of scheduling from the operating system. FLASH is able to make scheduling decisions in much fewer cycles as compared to modern operating system schedulers. The scheduling decisions it makes are just as good as those that would be made in software, but in much reduced time and without negatively impacting processor performance features. FLASH is designed to keep kernel modifications minimal, only requiring changes when the kernel scheduling interface changes.
\end{abstract}


\section{Introduction}
% \lipsum[1]
Traditional desktop operating systems are interactive, meaning that they serve as the primary interface between a user and a machine. Traditional server operating systems are batch-job oriented, meaning that processes are not pre-empted: a process continues until it exits or is forced to exit due to error. This preemption is one of the performance factors that contribute to greater process utilization on server operating systems as opposed to desktop operating systems. The reason why preemption is detrimental to CPU utilization is a combination of a few different reasons.

\paragraph{TLB and Cache}
The translation lookaside buffer (TLB) is a hardware structure that assists the memory management unit (MMU) of the CPU in translating virtual addresses into physical addresses. The TLB serves as a cache to look up commonly used virtual addresses and return the corresponding physical address. In computationally expensive portions of code, this unit is advantageous as it prevents unnecessary page lookups when the code is accessing a small region of memory. Likewise, the cache on a CPU provides good speedups to this kind of programs, as well as programs that have good data locality.

These structures work very well so long as the same process is in memory. When a context switch, changing from one process to another, is performed, the values of the cache and TLB are essentially invalidated. The new resident process must now wait for expensive memory accesses to bring in valid TLB entries and cache values. Before that process is even loaded, a scheduling algorithm must first decide which process to run next. This results in less than ideal processor utilization.

It is also difficult to make real time scheduling guarantees. In applications such as video playback, it is necessary that the playback process be scheduled consistently to ensure jitter-free playback. FLASH is able to make these guarantees due to its asynchronous computation and throughput. Additionally, FLASH minimizes the penalty to cache and TLB invalidation due to scheduler activity, while providing a complete scheduling interface.

\section{Scheduling in the Kernel}
% Mark
% \lipsum[1]

Our research focuses on the open source Linux kernel. In the Linux kernel, the Completely Fair Scheduler (CFS) has been the default scheduler since kernel release 2.6.23 \cite{cfsdesign}. It operates in $ O(log\ n) $ time and is designed to make a real CPU emulate an ideal multi-tasking CPU as much as possible. The CFS scheduler's design is a large part of why it continues to be the default scheduler on most Linux distributions.

\subsection{CFS Design}
The goal of the CFS is simple; pick the task with the shortest runtime next. In order to be a true multi-tasking scheduler, all processes should run for the same amount of time. A simple approach to this goal is a round-robin scheduler, where each task is run for a pre-determined amount of time, deemed a slice, whether or not it uses all of the time in its slice. The $O(n)$ scheduler used such a design, but it suffered from poor performance as the number of processes increased.

\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/mutlitasking.jpg}
		\caption{
			Ideal multi-tasking. Taken from \protect\cite{fig:multitask}.
		}
		\label{fig:cfs_multitask}
	\end{center}
\end{figure}

The CFS is able to keep performance minimal by making use of red-black trees. These are a type of self-balancing sorted tree, and the CFS uses them to keep track of task runtime. This means that locating the task with the least runtime is as simple as traversing to the smallest node in the tree. This yields good interactivity and multi-tasking because all processes eligible to run will eventually be run within a round, unlike with the $O(1)$ scheduler that CFS eventually replaced, which used complex heuristics that were subject to miscalculations.

\subsection{Performance}
The runtime statistics that CFS basis its decision on is part of the \verb|task_struct| structure, which is the representation of a process in Linux. Due to the dynamic number of tasks, these structures are located in the virtual region of kernel memory, and thus can cause the TLB to evict pages, reducing the performance of the previous resident application if that application will be elected to run again, which is common on multi-processor machines. Additionally, tree traversal does not exhibit good cache locality, which will generate cache misses within the scheduler algorithm itself, as well as the former process itself, if it is elected to run again.

Compared to the previous scheduler implementations, CFS does a much better job of true multi-tasking. While these factors are difficult to mitigate from a software approach, they are much more feasible to ameliorate using a software-hardware hybrid approach, along the same idea of the MMU.

\section{FLASH Architecture}
The FLASH hardware unit is built with ease of replacement of the usual
software implementation in mind.  The major addition we add over a standard
scheduler is the ability for the scheduler itself to raise an interrupt to
indicate a tick. Figure~\ref{fig:arch_overview} gives an overview of the
system architecture.

\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/flash-diagram.png}
		\caption{
			FLASH System Architecture: Interface.  Note the distinction between
			the \emph{Process Control} and \emph{Scheduling Control} Interfaces.
		}
		\label{fig:arch_overview}
	\end{center}
\end{figure}

We now describe in more detail both the interface and
internal implementation of the scheduling unit.

\subsection{FLASH Interface}
At its most basic, the hardware interface is given by two major segments:
\emph{scheduling control} and \emph{process control}.  Together these two
segments allow processes to be scheduled based on an up-to-date accounting
of process status, kept via the process control interface.  The scheduling
controller interface sends timer tick interrupts and serves incoming
requests for new processes to run.

\subsubsection{Process Control Interface}
Let us first focus deeply on this process control interface.  It is here
that FLASH is given information to hold the proper state of all running
processes.  Currently, we store process id (PID), priority, and state
triples as passed through the interface using a standard four-phase
handshake.

\paragraph{Consistency} A major hurdle with offloading scheduling decisions
to a dedicated piece of hardware is ensuring consistency between the data
structures maintained by the software kernel (again, here Linux) and the
hardware unit.

We cannot completely offload the data about processes to hardware most
obviously because of sheer size of objects like the \ttt{task\_struct} as
well as their high use by modules other than the scheduler.  We must,
though, retain some structure in FLASH memory so that it may make even the
most basic scheduling decisions.

For this reason, we expect each update in process priority or state to be
communicated to FLASH so it may continue making accurate scheduling
decisions.

\subsubsection{Scheduling Control Interface}
In addition to the process control interface, FLASH provides a scheduling
control interface, responsible for providing the software with tasks to be
run.  We can further subdivide this interface into two subparts: scheduling
requests and timer ticks.  Making replacement of software more
straightforward, these subparts directly correspond to the two main modes of
tasks switching in the Linux Kernel.

\paragraph{Scheduling Requests}
Often a running process will want to do something (such as disk I/O) that
requires yielding the processor.  When this occurs, the Linux kernel will
call \texttt{schedule()}.  We modify this schedule function to interact with
our scheduling control interface, more specifically the portion dealing with
scheduling requests.

A scheduling request will be received using a standard four-phase handshake.
At this point, using methods explained in Section~\ref{sec:FLASH_impl}, the
next task to be run is selected and returned to the software.

\paragraph{Timer Tick}
FLASH also provides functionality to raise an interrupt via a timer tick.
For simplicity and fully encapsulating scheduling logic, we export the
generation of the timer tick to this hardware unit.  At a specified time
interval, FLASH will raise a timer interrupt and provide the software with
the next task to be run.

\subsection{FLASH Implementation}
\label{sec:FLASH_impl}
FLASH aims to implement a CFS scheduling algorithm\footnote{The current
implementation is a prioritized round robin algorithm.}, allowing
transparent replacement of the current software implementation.  Internally,
we have two front-end modules, each corresponding to a segment of the
interface (process control and scheduling control). Those front-end modules
connect to back-end modules which regulate and control access to the backing
internal data structures.  We see this internal implementation architecture
described in Figure~\ref{fig:impl_overview}.

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{fig/flash-impl.png}
		%\includegraphics[width=0.9\linewidth]{fig/flash-impl.png}
		\caption{
			FLASH System Architecture: Implementation.  Notice the division
			between the front-end, back-end, and process data modules.  This
			allows tightly regulated access to backing process data store.
		}
		\label{fig:impl_overview}
	\end{center}
\end{figure*}

\subsubsection{Front-End Modules}
As mentioned above, we have a front-end module for each segment of the
interface.  Each of these modules is quite simple.  The module corresponding
to the scheduling control interface has two jobs: responding to scheduling requests
as well as passing on timer ticks generated by a different internal module.
Both of these require querying for the next task to be run.

The second front-end module corresponds to the process control interface.
It will simply communicate through the incoming request and update the
backing store of process state.

\subsubsection{Back-End Modules}
The FLASH back-end modules serve to regulate access to the backing data
store.  This is done via two modules: a reading module and a writing module.
The process control datapath is connected through the writing module as this
interface is used to modify the state of processes.

Additionally, we have a simple reading module connected to the scheduling
control interface.  This regulates reads to the backing data store in order
to determine the next process that ought to be run.

\subsubsection{Storing Process State}
The current FLASH implementation is a basic version of CFS.  We do not use
the same type of implementation as the Linux kernel because implementing
red-black trees in hardware would be prohibitively complex.  Instead we use
a series of simple queues that are sorted with priority in mind.

The FLASH scheduling algorithm, like CFS, will keep record of the runtime of
the process as well as a \emph{virtual runtime}.  This virtual runtime is a
weighted accounting of the actual runtime based on priority of the process.
Scheduling decisions are made based on the virtual runtime, as in CFS.

\paragraph{On Red-Black trees}
While using a red-black tree would allow the implementation to be
algorithmically faster, actual runtime in software will likely be higher
than iterating through a simple queue implemented in hardware.  Firstly,
hardware will simply be faster than software as it is more easily
parallelizable.  The hardware can parallelize comparisons in logarithmic
time, whereas an iteration in software will take linear time.

Secondly, traversing the red-black tree in software will not have ideal cache
performance. Switching to a self-contained hardware module will not have any
cache considerations because that hardware unit will have a single
monolithic memory.

\paragraph{What do we store?}
In addition to storing the PID, priority, and state, we also have a few
fields used to store internal state necessary to keep the CFS implementation
completely fair.  These fields most importantly include a notion of virtual
runtime.  Other fields are for bookkeeping reasons.

\subsection{Summary}
Together the interface and implementation of FLASH provide an easily
integrable scheduling module.  The kernel will interface with the hardware
unit in the same way that the software scheduler does.  Additionally, FLASH
implements the current default Linux scheduler CFS.

\section{Evaluation}
% Mark
% \lipsum[1]

There are a few metrics by which we compare scheduler performance. We are mostly concerned with CPU involvement under normal operating conditions, but as the entry point of the two schedulers differ, the point of measurement differs. In the case of CFS, we analyze how many cycles \verb|pick_next_task| requires, since the kernel is already optimized for CFS. In the case of FLASH, we measure how long the DMA read takes. It would be disingenuous to consider how long a read from memory takes, as the driver duties of FLASH also consume CPU time that are critical to scheduling duties that CFS does not require.

We do not currently have these metrics for our CARGO implementation due to a clock measurement issue on our test machine that is currently being investigated. We also do not have these metrics for our SoC implementation as that is still in progress.

An additional metric is cache and TLB misses and overall cycle count, attainable through the use of the \verb|perf| tool. We provide the results for the CFS scheduler as of Linux 3.19.2 (Table \ref{table:perf}); we will also provide the SoC results upon completion.

% Please add the following required packages to your document preamble:
\begin{table*}[t]
	\begin{center}
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Implementation & Perf Cycles ($10^6$) & Self Reported Cycles & Context Switches & \% Cache Misses \\ \midrule
			CFS            & 7.23                               & 4.02                 & 610              & 23              \\
			FLASH (SoC)    & TBA                                & TBA                  & TBA              & TBA             \\ \bottomrule
		\end{tabular}
		\caption{FLASH vs CFS for a busy wait}
		\label{table:perf}
	\end{center}
\end{table*}

We will also give measurements of the overhead involved with communicating with FLASH hardware, in terms of process creation, destruction, and other state changes. By contrast, CFS also has similar overhead in terms of task time accounting.

\section{Integration}
% Chae then Mark
FLASH has been developed using SystemC and High Level Synthesis (HLS)
techniques.  This allows the engineer to work at the algorithmic level of
abstraction.  Not only prototyping but also verification can complete at a
quicker pace at the system level.  Architectural tradeoffs can be fully
examined and explored at a quicker pace than with traditional RTL
development.  More potential designs can be analyzed, and a Pareto curve can
be generated.  Using that information, the most suitable design can be
chosen as a tradeoff between (generally) area and speed.

% more Mark stuff here

In order to rapidly develop FLASH, we began development using a hardware emulator developed at Columbia called CARGO \cite{cargo}. The framework provided allowed us to quickly develop our driver and verify basic functionality. CARGO also provides DMA semantics, allowing us to measure how many cycles from a hardware side of things DMA communication would require.

From CARGO, we move to an Altera System on Chip Field Programmable Gate Array, or just SoC FPGA. This device consists of an FPGA, a programmable block of hardware, interconnected with an ARM CPU. This device allows rapid prototyping of custom hardware and integration on a real system. We will use this SoC to determine how FLASH functions in a non-synthetic environment, and how applicable it would be in real devices.

\section{Related Works}
% Chae
One of the major bottlenecks to server-class performance in interactive
desktop environments is that of the context switch and in particular
scheduling.  FLASH attempts to provide a hardware module which serves as the
scheduler for the Linux Kernel.  We replace the CFS from the kernel with our
own implementation in accordance with the major principles of the Completely
Fair Scheduler \cite{wong2008cfs}.

Previous hardware scheduling units focus completely on embedded systems with
a fixed number of tasks \cite{nakano1995hardware, morton2004hardware,
nacul2007hardware, kuacharoen2003configurable, park2008hardware}.  The major
novel challenge in escalating to a desktop Linux is handling the dynamic,
unbounded (from a realistic hardware perspective) number of tasks.


\section{Conclusion}
% \lipsum[1]
In this paper, we present FLASH, a hardware scheduler accelerator designed to replace an operating system scheduler. FLASH performs scheduling decisions in hardware, thus freeing up the CPU and related structures to focus on computation. FLASH only interrupts the CPU when it has made a new decision, and only if that decision differs from the currently running task, thus reducing unnecessary interruptions. We will show the viability of FLASH hardware as an accelerator akin to the MMU by considering its power requirements and area usage.

\nocite{*}
{
	\bibliographystyle{abbrv}
	\bibliography{ref}
}

\end{document}
