\documentclass{sig-alternate-10pt}

\newcommand{\ttt}{\texttt}

\usepackage{graphicx}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{booktabs}

\title{FLASH\\Fast Linux Advanced Scheduler Hardware}
\author{
	Mark Aligbe \\
	    \email{ma2799@columbia.edu}
	\and
    Chae Jubb \\
        \email{ecj2122@columbia.edu}
}
\date{4 May 2015}

\begin{document}
\maketitle

\begin{abstract}
As accelerators become more common and necessary as a way to continue Moore's Law in the absence of Dennard Scaling, researchers explore segments of computation that can be improved with the aid of dedicated hardware. This paper presents FLASH, a hardware scheduler to take over the task of scheduling from the operating system. FLASH differs from previous hardware schedulers in that it implements a time sharing algorithm, as opposed to priority based or real-time scheduling. FLASH is able to make scheduling decisions in much fewer cycles as compared to modern operating system schedulers. The scheduling decisions it makes are equivalent to those made in software, but in much reduced time and without negatively impacting processor performance features. FLASH is designed to keep kernel modifications minimal, only requiring change when the kernel scheduling interface changes.

\end{abstract}


\section{Introduction}
\label{sec:intro}
Traditional desktop operating systems are interactive, meaning that they serve as the primary interface between a user and a machine. Traditional server operating systems are batch-job oriented, meaning that processes are not pre-empted: a process continues until it exits or is forced to exit due to error. This preemption is one of the performance factors that contribute to greater process utilization on server operating systems as opposed to desktop operating systems. The reason why preemption is detrimental to CPU utilization is a combination of a few different reasons.

\paragraph{TLB and Cache}
The translation lookaside buffer (TLB) is a hardware structure that assists the memory management unit (MMU) of the CPU in translating virtual addresses into physical addresses. The TLB serves as a cache to look up commonly used virtual addresses and return the corresponding physical address. In computationally expensive portions of code, this unit is advantageous as it prevents unnecessary page lookups when the code is accessing a small region of memory. Likewise, the cache on a CPU provides good speedups to this kind of programs, as well as programs that have good data locality.

These structures work very well so long as the same process is in memory. When a context switch, changing from one process to another, is performed, the values of the cache and TLB are essentially invalidated. The new resident process must now wait for expensive memory accesses to bring in valid TLB entries and cache values. Before that process is even loaded, a scheduling algorithm must first decide which process to run next. This results in less than ideal processor utilization.

It is also difficult to make real time scheduling guarantees. In applications such as video playback, it is necessary that the playback process be scheduled consistently to ensure jitter-free playback. FLASH is able to make these guarantees due to its asynchronous computation and throughput. Additionally, FLASH minimizes the penalty to cache and TLB invalidation due to scheduler activity, while providing a complete scheduling interface.

\section{Scheduling in the Kernel}
\label{sec:sched_in_kernel}
% Mark
Modern desktop operating systems have to run hundreds of tasks, while providing a responsive interface to the user. This means that the CPU must be able to serve many interrupts a second, from various hardware devices, and service dozens or hundreds of background tasks while presenting a lagless experience to the end user. Each of these sources introduce potential sources of latency: interrupt handling \cite{regehr2007safe} requires many techniques in both hardware and software implementations to maintain relatively cheap and efficient scheduler design is an ongoing field of research \cite{wong2008cfs} \cite{park2008hardware} \cite{morton2004hardware}. We chose to focus on scheduling for our research as the potential benefits of scheduling in hardware in an operating system are not explored in the context of a desktop workload.

\subsection{History of Linux Schedulers}
The linux scheduler has gone through a few major design changes in its scheduler. Before the introduction of the $ O(n) $ scheduler in Linux 2.4, scheduler implementations simple and fast as CPUs themselves had not become as complex as compared to modern CPUs with simultaneous multi-threading (SMT) and symmetric multi-processing (SMP). These implementations were not scalable to multiple processors, so the $ O(n) $ scheduler was introduced to solve the problems introduced by SMP/SMT. The $ O(n) $ scheduler worked well so long as the number of tasks remained low, but as it had a single runqueue for all CPUs in a system, scaled poorly as the number of CPUs grew and as the number of processes grew (since it must iterate through the list of all processes to select a candidate).

The $ O(1) $ scheduler was introduced to solve the problem of large number of tasks. As the name implies, it is able to select a task to run in constant time. The $ O(1) $ scheduler achieves this efficiency by exploiting per-priority \textit{active} and \textit{expired} arrays for tasks. Tasks that are eligible to run are placed in the \textit{active} array and once a task has completed running it is placed in the \textit{expired} array. Thus, selecting a task is as fast as dequeuing the head of the highest priority array. Once the \textit{active} array is empty, the \textit{active} and \emph{expired} arrays are swapped by simply changing pointer directions. Unlike the $ O(n) $ processor, it had per-CPU runqueues, meaning task scheduling did not stall other CPUs. The $ O(1) $ was effective at its job, but was not a fair scheduler, was difficult to maintain through major kernel revisions, and its use of heuristics to determine the interactivity of a process was not without flaw. The CFS scheduler was introduced in Linux 2.6 as a replacement to solve the interactivity issues of the $ O(1) $ scheduler and introduce a simpler, but efficient scheduler.

\subsection{Implementation of CFS}

\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/mutlitasking.jpg}
		\caption{
			Ideal multi-tasking. Taken from \protect\cite{fig:multitask}. Every task gets an equal share of CPU time.
		}
		\label{fig:cfs_multitask}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/rbtree.png}
		\caption{
			A sample red black tree taken fron \protect\cite{wiki:rbtree}. The choice of color is arbitrary, and the distinction is used to maintain the balancing property of the tree.
		}
		\label{fig:rbtree}
	\end{center}
\end{figure}

The CFS scheduler is conceptually simple; it aims to implement an ideal multi-tasking CPU \cite{cfsdesign}. An ideal mutli-tasking CPU assigns equal time to all processes assigned to it, which is essentially a restatement of round-robin scheduling \ref{fig:cfs_multitask}. As \emph{expired} and \emph{active} arrays are the basis of the $ O(1) $ scheduler, red black trees are the basis of CFS. Red black trees are a type of self balancing binary search tree, and in the case of CFS, are ordered based on the virtual runtime of processes, as defined by the number of nanoseconds in CPU residence. This type of accounting affords CFS greater granularity than the other process schedulers, as it has no notion of a timeslice. Instead, CFS simply chooses the task with the smallest total runtime (with weighting applied for priority) and schedules it to be run next. This allows CFS to emulate a round-robin scheduler while offering greater granularity. Additionally, CFS is conceptually simpler and has a much smaller code footprint, which is better for embedded environments, than the $ O(1) $ scheduler.

\subsubsection{Limitations}
CFS operates at $ O(log n) $ efficiency (inserting a task back into the red black tree), which is less efficient than the $ O(1) $ scheduler, but CFS achieves fairness by virtue of its implementation, without favoring any particular kind of task (e.g. interactive vs non-interactive).

\section{Related Works}
\label{sec:related_works}
Previous hardware scheduling units focus completely on embedded systems with
a fixed number of tasks each with fixed priority
\cite{kuacharoen2003configurable, morton2004hardware, nacul2007hardware, nakano1995hardware,
park2008hardware}.  Scheduling under these fairly restrictive constraints is
greatly simplified with respect to desktop scheduling as there is a much
more defined ordering between tasks.

Additionally, there is often a small number of tasks relative to a desktop
environment.  This means that much simpler algorithms may be used in an
attempt to approximate the optimal task ordering.

The major novel challenge in escalating to a desktop Linux is handling the
dynamic, unbounded (from a realistic hardware perspective) number of tasks.
The initial FLASH prototype addresses this in an unsatisfying way: by
arbitrarily fixing the number of tasks that can be managed with its
scheduling policy.  As discussed in detail in Section~\ref{sec:future},
future iterations will remove this restriction by reserving a small section
of memory.

Because we aim to support an unbounded number of tasks, we must implement
a scheduler comparable to the default CFS currently used by the kernel.
While a simpler implementation than the kernel default, the FLASH scheduler
is implemented in accordance with the major principle of CFS
\cite{wong2008cfs}.

We should also consider the architecture of previous hardware schedulers.
Many of the previously cited works were designed either as ASICs
(Application Specific Integrated Circuits) or on FPGAs (Field Programmable
Gate Arrays) rather than a general purpose CPUs.  This makes scheduling of
resource usage slightly different in that there is not always an overhead of
a context switch: we may simply be deciding which process can use
a particular multiplier.


\section{FLASH Architecture}
\label{sec:arch}
% Chae
The FLASH hardware unit is built with ease of replacement of the usual
software implementation in mind.  The major addition we add over a standard
scheduler is the ability for the external scheduler itself to raise an
interrupt to indicate a tick. Either at this tick or when a process is
requested, FLASH will provide the process ID of the task to the run next.
Figure~\ref{fig:arch_overview} gives an overview of the system architecture.

\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/flash-diagram.png}
		\caption{
			FLASH System Architecture: Interface.  Note the distinction between
			the \emph{Process Control} and \emph{Scheduling Control} Interfaces.
		}
		\label{fig:arch_overview}
	\end{center}
\end{figure}

\subsection{Improvements}
FLASH provides improvements over the CFS software solutions.  The most
notable potential advantage is in speed.  Having a separate hardware unit
allows the CPU to be running tasks at the same time the choice of next task
is being prepared.  This way it can be immediately read without having to
wait for a computation.

Additionally, data structures are maintained on a separate data path.  This
means that scheduling computations will not pollute the usual cache
hierarchy or TLB.  As cache misses are costly, eliminating a small class of
them will directly correlate to a performance increase.  This will be
especially true as modern processor hardware (newer versions of x86 and ARM)
have more effective TLBs that are not, for example, completely flushed on
a context switch\cite{neiger2006intel}.  FLASH allows program data a larger
share of the cache hierarchy and TLB.

Increasingly, desktop machines are being used as virtualization platforms.
The problem of TLB hit rate is especially relevant with the added number of
virtual memory spaces so it is important that the TLB stay as intact as
possible through context switches.

\subsection{Scheduling Algorithm}
As previously mentioned, in order to best schedule the set of eligible
tasks, we use the same fundamental algorithm to which the Linux kernel
defaults: the Completely Fair Scheduler.  The kernel's implementation
fundamentally relies upon red-black trees.

However, implementing a red-black tree in hardware with a fixed amount of
memory would be prohibitively expensive.  For this reason, the FLASH
implementation utilizes a simple unordered array.  Because there are
typically very few tasks, the cost of keeping and constantly reorganizing an
ordered data structure would would outweigh a simple iteration.  Further, we
must importantly note that this lookup in hardware will be much quicker than
in software, given that the data is stored in a single monolithic,
low-latency memory.  Even more convincing is the ease with with
parallelization of lookup can be implemented using hardware.

FLASH does, however, retain the core functionality of the CFS basis
algorithm.  Scheduling decisions are made using the notion of virtual
runtime.   Just as in the Linux implementation, virtual runtime is
calculated as a weighted physical runtime based upon the priority of the
task.  FLASH uses the same relative weighting as Linux.

We now describe in more detail both the interface and internal
implementation of the scheduling unit.

\subsection{FLASH Interface}
At its most basic, the hardware interface is given by two major segments:
\emph{scheduling control} and \emph{process control}.  Together these two
segments allow processes to be scheduled based on an up-to-date accounting
of process status, kept via the process control interface.  The scheduling
controller interface sends timer tick interrupts and serves incoming
requests for new processes to run.

\subsubsection{Process Control Interface}
Let us first focus deeply on this process control interface.  It is here
that FLASH is given information to hold the proper state of all running
processes.  Currently, we store process id (PID), priority, and state
triples as passed through the interface using a standard four-phase
handshake.

\paragraph{Consistency} A major hurdle with offloading scheduling decisions
to a dedicated piece of hardware is ensuring consistency between the data
structures maintained by the software kernel (again, here Linux) and the
hardware unit.

We cannot completely offload the data about processes to hardware most
obviously because of sheer size of objects like the \ttt{task\_struct} as
well as their high use by modules other than the scheduler.  We must,
though, retain some structure in FLASH memory so that it may make even the
most basic scheduling decisions.

For this reason, we expect each update in process priority or state to be
communicated to FLASH so it may continue making accurate scheduling
decisions.

\subsubsection{Scheduling Control Interface}
In addition to the process control interface, FLASH provides a scheduling
control interface, responsible for providing the software with tasks to be
run.  We can further subdivide this interface into two subparts: scheduling
requests and timer ticks.  Making replacement of software more
straightforward, these subparts directly correspond to the two main modes of
tasks switching in the Linux Kernel.

\paragraph{Scheduling Requests}
Often a running process will want to do something (such as disk I/O) that
requires yielding the processor.  When this occurs, the Linux kernel will
call \texttt{schedule()}.  We modify this schedule function to interact with
our scheduling control interface, more specifically the portion dealing with
scheduling requests.

A scheduling request will be received using a standard four-phase handshake.
At this point, using methods explained in Section~\ref{sec:FLASH_impl}, the
next task to be run is selected and returned to the software.

\paragraph{Timer Tick}
FLASH also provides functionality to raise an interrupt via a timer tick.
For simplicity and fully encapsulating scheduling logic, we export the
generation of the timer tick to this hardware unit.  At a specified time
interval, FLASH will raise a timer interrupt and provide the software with
the next task to be run.

\subsection{FLASH Implementation}
\label{sec:FLASH_impl}
FLASH aims to implement a CFS scheduling algorithm\footnote{The current
implementation is a prioritized round robin algorithm.}, allowing
transparent replacement of the current software implementation.  Internally,
we have two front-end modules, each corresponding to a segment of the
interface (process control and scheduling control). Those front-end modules
connect to back-end modules which regulate and control access to the backing
internal data structures.  We see this internal implementation architecture
described in Figure~\ref{fig:impl_overview}.

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{fig/flash-impl.png}
		%\includegraphics[width=0.9\linewidth]{fig/flash-impl.png}
		\caption{
			FLASH System Architecture: Implementation.  Notice the division
			between the front-end, back-end, and process data modules.  This
			allows tightly regulated access to backing process data store.
		}
		\label{fig:impl_overview}
	\end{center}
\end{figure*}

\subsubsection{Front-End Modules}
As mentioned above, we have a front-end module for each segment of the
interface.  Each of these modules is quite simple.  The module corresponding
to the scheduling control interface has two jobs: responding to scheduling requests
as well as passing on timer ticks generated by a different internal module.
Both of these require querying for the next task to be run.

The second front-end module corresponds to the process control interface.
It will simply communicate through the incoming request and update the
backing store of process state.

\subsubsection{Back-End Modules}
The FLASH back-end modules serve to regulate access to the backing data
store.  This is done via two modules: a reading module and a writing module.
The process control datapath is connected through the writing module as this
interface is used to modify the state of processes.

Additionally, we have a simple reading module connected to the scheduling
control interface.  This regulates reads to the backing data store in order
to determine the next process that ought to be run.

\subsubsection{Storing Process State}
The current FLASH implementation is a basic version of CFS.  We do not use
the same type of implementation as the Linux kernel because implementing
red-black trees in hardware would be prohibitively complex.  Instead we use
a series of simple queues that are sorted with priority in mind.

The FLASH scheduling algorithm, like CFS, will keep record of the runtime of
the process as well as a \emph{virtual runtime}.  This virtual runtime is a
weighted accounting of the actual runtime based on priority of the process.
Scheduling decisions are made based on the virtual runtime, as in CFS.

\paragraph{On Red-Black trees}
While using a red-black tree would allow the implementation to be
algorithmically faster, actual runtime in software will likely be higher
than iterating through a simple queue implemented in hardware.  Firstly,
hardware will simply be faster than software as it is more easily
parallelizable.  The hardware can parallelize comparisons in logarithmic
time, whereas an iteration in software will take linear time.

Secondly, traversing the red-black tree in software will not have ideal cache
performance. Switching to a self-contained hardware module will not have any
cache considerations because that hardware unit will have a single
monolithic memory.

\paragraph{What do we store?}
In addition to storing the PID, priority, and state, we also have a few
fields used to store internal state necessary to keep the CFS implementation
completely fair.  These fields most importantly include a notion of virtual
runtime.  Other fields are for bookkeeping reasons.

\subsection{Summary}
Together the interface and implementation of FLASH provide an easily
integrable scheduling module.  The kernel will interface with the hardware
unit in the same way that the software scheduler does.  Additionally, FLASH
implements the fundamental algorithm of the current default Linux scheduler
CFS, relying upon the notion of a priority-weighted virtual runtime.


\section{Integration}
\label{sec:integration}
% intro stuff here
\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/hlsctos.jpg}
		\caption{
			The SystemC workflow. The design is written using SystemC, and then \"scheduled\", or compiled to RTL using specified design paramteres and the CtoS compiler.
		}
		\label{fig:ctos_overview}
	\end{center}
\end{figure}

In testing FLASH, a platform that enables rapid prototyping is essential. There are two components to prototyping FLASH; however. The first is the choice of devlopment language. While the software component must be written in C, there are many choices for a hardware descriptive language. In order to quickly iterate on the design of FLASH, a high level hardware descriptive language was chosen, SystemC. Unlike Verilog and VHDL which specify hardware at essentially the Register Transfer Level (RTL), SystemC allows designs to be specified in \"hardware\" aware C/C++. Designs can be written using C/C++ syntax and a basic notion of time flow, but the code style is still oriented towards hardware limitations, excluding constructs like pointers, dynamic memory allocation, dynamic stack values, and things generally made possible by having a large standard library and a large memory bank. We use Cadence C-to-Silocon (CtoS) to compile our SystemC code to RTL, which is then used in our second prototyping component.

Since our design is a hybrid between custom hardware and software, we need a device that is capable of being dynamically reconfigured combined with a Linux-capable processor. Such a platform is the Arrow SoCKiT, powered by Altera's Cyclone V.

\subsection{Cyclone V}
% Mark
\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/sockit-architecture.png}
		\caption{
			Arrow SoCKiT high level overview. The embedded ARM processor is connected to the FPGA fabric via a ARM's AMBA AXI interface, on top of which runs Altera's Avalon MM interface.
		}
		\label{fig:sockit_overview}
	\end{center}
\end{figure}

The Cyclone V family of Field Programmable Gate Arrays (FPGA) also include System on Chip (SoC) configurations. In these configurations, the FPGA is connected to a hard processor system (HPS) realized by an ARM Cortex A9, as shown in \ref{fig:sockit_overview}.

\subsubsection{Quartus}
The first step in realizing our design is synthesizing the design using Altera Quartus II. Quartus is Altera's IDE for designing and verifying RTL descriptions targeted at their FPGAs. Once the design is compiled to RTL using CtoS, it is then configured in Quartus using Qsys, which is a system integration tool also developed by Altera that facilitates the task of connection complex modules to complex systems. In order to communicate with the HPS, hardware modules are connected to the Avalon MM bus which is implemented on top of ARM's AMBA AXI interface for communication with the ARM processor. Once the design is connected using Qsys, it is synthesized using Quartus, and then a bitstream used to program the FPGA is generated.

\subsubsection{UBoot and Kernel}
\begin{figure}
	\begin{center}
		%\includegraphics[width=0.65\textwidth]{fig/flash-diagram.png}
		\includegraphics[width=0.9\linewidth]{fig/soc_cyclonev.png}
		\caption{
			Arrow SoCKiT high level overview. The embedded ARM processor is connected to the FPGA fabric via a ARM's AMBA AXI interface, on top of which runs Altera's Avalon MM interface.
		}
		\label{fig:cyclone_diag}
	\end{center}
\end{figure}

U-Boot is a bootloader targetted for embedded devices, similar to GRUB for desktop Linux computers. U-Boot differs from GRUB in that its boot paramters are included in the generated binary for the target device, specifying the location of the kernel image and a special file called the Device Tree Blob (DTB). The DTB is essentially a binary form of the Device Tree Source (DTS), which specifies all the hardware available to the processor on the embedded device, and their address location and size. Support for generating device tree blobs is part of the kernel \cite{dts_linux}, and provides a simple mechanism for embedded developers to make their hardware available to the associated processor. We use the dts for the Cyclone V provided by RocketBoard.org, who provide linux sources and tools for various embedded devices, and modify it to include the \"lightweight bus\", or the Avalon MM interface, and specify the address of our \"flash\" device.

The kernel image is cross built using the GCC ARM v7 toolchain, using the sources from rocketboards.org configured for the Cyclone V SoCKiT. These sources contain branches for a variety of SoC devices as well as configurations for accessing their hardware, with the Cyclone V layout shown in Figure \ref{fig:cyclone_diag}. Then to boot Linux on the SoC, the SoCKiT must be configured to boot from the appropriate source, either over ethernet using BOOTP or as in our case, from a micro SD card. The U-Boot image is used to bootstrap the Linux kernel \emph{uImage} also located on that same source. 

\subsubsection{Ubuntu Linaro}
\label{subsubsec:linaro}
However, a root filesystem still needs to be provided. There are two main choices; a Buildroot distribution, which is a distribution targeted specifically for embedded devices. We will talk more about Buildroot in Section \ref{sec:eng_exp}. The option we chose was Ubuntu Linaro, a distribution of Ubuntu targeted for ARM devices. Since the Cyclone V contains a relatively powerful ARM processor, it is capable of running a full desktop Linux distribution backed by the micro SD card. We use this as our basis of evaluation to determine the relevance of FLASH in a practical device such as a smartphone or Chromebook. The Ubuntu Linaro distribution also has access to a larger collection of software, such as Xorg and gcc, both of which were to be part of our evaluation. The Linaro distribution can be downloaded as an archive containing the entire root filesystem, and then extracted to whichever source the kernel will be booted from.

\subsection{Kernel Modifications}
% Chae
In order to actually take advantage of our custom hardware, we need to
inform the operating system---here, Linux---that the new device is available
to be used.  This requires creating a device driver to communicate with the
device, and, additionally, a Linux scheduling class and policy to use it as
a scheduler.

\subsubsection{FLASH Device Driver}
The primary purpose of the FLASH device driver is to facilitate
communication between the Linux kernel and the FLASH hardware device.
Because the possible types of communication is quite limited, the device
driver is fairly simple.  The kernel either asks for a task to schedule or
provides an update about process status.  In the opposite direction, the
device only provides the next tasks to be run as well as raising an
occasional interrupt.

Upon initialization, the driver will register the device and then allocate
a memory region to be used as internal device registers.  Finally, we must
register an interrupt handler to handle the interrupt generated by the
device upon each timer tick.

Once the device has been initialized, the driver will simply serve requests
(as instructed by the scheduling policy) via a series of \texttt{iowrite32}
and \texttt{ioread32} function calls.  This will come in addition to
handling device-generated interrupts and passing them up through the stack.

Device destruction is simply the reverse of device initialization.  When the
device is in the destroyed state, the FLASH scheduling policy may not be
used as reads from and writes to the device will fail.

\subsubsection{FLASH Scheduling Policy}
Now that we have constructed a device driver and can communicate with the
FLASH device, we must implement a scheduling policy that allows us to use
the novel device as a scheduler.

Implementation of this policy is done using the usual scheduling interface
provided by the Linux kernel.  We construct a \texttt{struct sched\_class}
and the implementation of the relevant core scheduling functions such as
\texttt{enqueue\_task}, \texttt{dequeue\_task}.  We implement the provided
interface in a way that is a thing wrapper around calls to device driver
functions.  This is because the algorithmic work is done in the device, so
we need only relay decisions made by the device to the core scheduler.

After the scheduling class has been implemented, we must modify the core
scheduler itself to make FLASH the default scheduling policy.  This way all
tasks will be scheduled using the FLASH policy unless explicitly specified
otherwise.  As we are implementing a replacement scheduler for CFS, we place
our scheduling policy at a higher priority than CFS.  However, we keep it
below policies such as real-time.

\paragraph{Scheduling Policy Priorities}
The available scheduling classes in the Linux kernel are kept in
a prioritized linked list.  Here priority refers to the ordering in which
scheduling policies will be asked for a tasks to next run.  A higher
priority policy, such as real-time, will always be able to run a task if it
has one in the runnable state, even if lower priority policies such as idle
have multiple tasks in the runnable state.

\subsubsection{Kernel space Interface}
To further abstract the implementation of the scheduling policy, a pleasant
interface is provided to the user were she to use standard kernel functions
functions to control the behavior of the scheduler in the non-default way.
This includes updating process priority or state of those processes on the
FLASH scheduling policy.  A \texttt{struct} with these relevant fields is
provided so that only the driver need worry about the bit ordering that the
device itself expects to receive.  This interface, for example, would allow
users of both little endian and big endian a generic interface in a similar
way the socket API provides functions such as \texttt{htonl} and
\texttt{ntohs}.


\section{Applications}
\label{sec:apps}
% Mark
Our intention in developing FLASH was to produce a device that would ultimately find adoption in heterogeneous computing platforms (like the Apple \"A Series\"), where specialized function cores are used to accelerate the function of a CPU. We also believe that our device can be relevant as an accelerator unit in modern monolithic CPUs.

\subsection{MMU-like Device}
\label{subsec:mmu}
The Memory Management Unit is a fundamental part of all processors with large amounts of RAM. Its task is to keep accesses to virtual memory fast and correct, and to alleviate many of the memory performance issues associated with multi-tasking operating systems and large address spaces. Without the MMU, the virtual memory abstraction provided by operating systems would result in multiple access for every single memory access a program makes. The TLB is the core of the MMU and its task is to cache a fixed number of virtual memory translations. The size of the cache depends on the hardware, but for the Cortex A9, it contains up to 4 by 32 entries, resulting in at least 8192 bits of storage, assuming 32 bit addresses \cite{arm_mmu}.

The information for the total number of gates an MMU requires in a given CPU architecture is not very forthcoming, but the factor that normally dominates a design size is the memory usage. Our design by comparison uses 8.5 kb of storage, which is directly comparable to the memory usage of the Cortex A9 MMU.

\subsection{Obstacles to Adoption}
The main reason why the MMU is present on any desktop operating system processor is that its performance boost is crucial to keeping these devices running fast, as explained in \ref{subsec:mmu}. There are two improvements FLASH could potentially bring, and need to be proven in order for any sort of adoption to take place, one feature that must be further researched, which we talk about in \ref{subsec:flash_size}, and a final aspect which must be addressed.

\subsubsection{Performance}
The first improvement is performance improvements; as the MMU makes memory access fast, FLASH must also demonstratebly make scheduling fast. The acceptable level of \"fast\" likely depends on the target workload. Perhaps it is fine if FLASH is not much faster so long as it provides significant energy efficiency over computing in software. Naturally, slower CPUs will take longer to make a scheduling decision, but FLASH is plenty on a 50 MHz clock, with 50,000 cycles to make a scheduling decision at a 1 kHz clock speed. Thus, FLASH is less sensitive to processor speed than processors are in the context of making scheduling decisions. For this reason, we believe that low power systems would be ideal candidates for FLASH adoption.

\subsubsection{Energy Savings}
Some low power systems (like smartphones) are as complex, if not more so, than laptops and desktops. In the case of smartphones, many of them contain dedicated Intellectual Property cores (IP) that performs certain tasks, like network management, video acceleration, usb communication, etc. These IP cores typically free up the CPU from performing a processing intensive task that it either cannot perform (WiFi communication) or would be tedious (SPI communication). We believe that FLASH can benefit such a system. FLASH would free up the CPU on smartphones to service the interrupts it constantly receives instead of repeating the mundane task of scheduling the many background tasks that run on thse devices.

\subsubsection{Infinite Tasks}
Since FLASH is a hardware construct, it has a finite amount of memory. Thus, it can only support a maximum number of tasks. Since threads are also schedulable entities, this maximum number can be reached relatively easily on a desktop Linux environment. Thus, it will be necessary for FLASH to maintain structures in main memory in order to support an unbounded number of tasks. The semantics of this management might consist of evicting tasks that are unlikely to run anytime soon because they have already run for too long.

\section{Engineering Experiences}
\label{sec:eng_exp}
% both
The road to Ubuntu Linaro on the SoCKiT with a communicable FLASH device was fraught with many hardships. We discuss the work done leading up to this development.
% \lipsum[1-3]
\subsection{DE2}
Our initial testing environment was the Terasic DE2, an FPGA development board powered by Altera's Cyclone II. It differs greatly from the Cyclone V in that it does not come with a hard processor system; instead one instantiates a NIOS II processor using Altera's tools. The problem was that the combination of the DE2 and the NIOS II processor was prohibitive to building custom kernels.

There were two main problems: booting a custom kernel and developing a NIOS II system.Our first attempt at running Linux on the DE2 was successful using precompiled images, but we would need to modify the kernel and system to add FLASH. Rather than modifying both at the same time, it would be more prudent to modify one at a time and verify that the system still functions. Attempts to build a custom kernel were successful, but booting the kernel was a separate matter. There were simply too many configuration items to inspect, and the documentation and tutorial for doing so all relies on repositories that are no longer in service.

In trying to synthesize a NIOS II system, one immediate issue was that all resources but one relied on the SOPC Builder, which is the precursor to Qsys. As none of the authors had experience with SOPC Builder, we used the Qsys approach, but were stalled by yet another file to be retrieved from a decommissioned repository. Attempts to regenerate the file and substitute it in the relevant commands yielded results that were not workable. Ultimately, the age of the DE2 was prohibitive to using it as our testbed, and thus we transitioned to the more modern Cyclone V, towards the end of our research.

\subsection{Buildroot}
Buildroot, as briefly discussed in \ref{subsubsec:linaro}, is a root filesystem for an embedded device. Building and installing buildroot was relatively straightforward. However, when it came to developing software on the device itself to speed up testing, Buildroot offers little options. In addition to lacking Xorg, it also had some routing issues that were prohibitive to getting the package manager to work. Additionally, the package manager did not seem to operate correctly as attempts to use any functions other than its help were met with blank output and a zero exit status. While it was simpler to build than Linaro, it was simply not functional and not representative of a true desktop Linux distribution.

\subsection{Acts of God}
One of this paper's authors experienced a few severe technical difficulties over the course of research. The largest incidents occured in mid March and early April, essentially costing this author three weeks of productivity. The incident in mid March was the malfunction of this author's last functioning laptop, at which point the author decided it would be prudent to correct his other malfunctioning laptops so that a similar situation would not arise in the future. The incident in early April was the theft of all of the authors laptops, which were recently made functional. The author has learned the anti-theft software is good insurance, and the author has also learned that it is important to synchronize one's work environment across all of one's available development machines.

\subsection{CtoS and RTL}
As previously mentioned, this undertaking used high level synthesis (HLS)
tools.  In order to use this design, we must synthesize onto the Altera
board.  This is traditionally done using Altera's proprietary Quartus suite
of software.  Thus, we need to bridge the gap between the HLS-ready SystemC
and RTL that can ultimately be synthesized.  In order to do this, we rely
upon Cadence's C-to-Silicon (CtoS) compiler.  However, CtoS requires that we
plug into a synthesis tool such as Quartus in order to perform area and
timing analysis for the design and construction of local memory blocks.

This process, however, was not straightforward.  Because CtoS only supports
using the subscription version of Quartus and authors did not have enough
institutional clout to have this expensive license installed on the same
server as the CtoS tool, the authors were forced to complete this
compilation to RTL using Quartus's main competitor: Xilinx's Integrated
Synthesis Environment (ISE).  This resulted in some non-optimized synthesis.

Overall, the experience of using CtoS was pleasant, especially as it allowed
some design-space exploration, making some area vs timing tradeoffs.
Because our design only pushed the area capacity of the board with a high
static task limit, most optimizations erred on the side of timing in order
to take full advantage of the board's speed.  There was some trouble
integrating these disjoint toolchains, but with a proper environment setup,
many of these problems would likely disappear.


\section{Future Work}
\label{sec:future}
While the framework of the FLASH scheduler is complete, it still needs to be tested and evaluated. Due to time constraints, we were not able to reach the evaluation phase. There are also two features that could potential improve the performance of FLASH that are not present in the current implementation.

\subsection{DMA}
\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{fig/dma.jpg}
		\caption{
			A block diagram of the function of DMA access, and how it frees up the processor from becoming involved in data transfers \protect\cite{fig:dma}.
		}
		\label{fig:dma_diag}
	\end{center}
\end{figure}
In Direct Memory Access (DMA), a controller handles the task of maintaining memory coherency with the processor while it is executing, as shown in \ref{fig:dma_diag}. This overhead is justified because the processor can continue executing while the controller interfaces with the device to read/store data. The advantage of DMA in the context of FLASH is that instead of performing an ioread/iowrite everytime a scheduler event occurs, the processor and device can use the DMA as a liason and access the data when they are ready.

This does not negate the necessity of retrieving a new process immediately if the current process calls \verb|sched_yield|, for example, but it reduces the number of ioreads/iowrites when there are plenty of processes in the running state.

\subsection{No HZ Interrupt}
Desktop Linux kernels use a timer to manage rescheduling needs. The speed of the timer directly corresponds to the interactivity of the kernel, with 1000 Hz yielding the most interactive kernel configuration supported, but with necessarily the poorest throughput. In transitioning to DMA, we can decide whether or not to raise interrupts after FLASH has made a scheduling decision. If our decision is the same as the currently running process, then there's no reason to raise an interrupt. This is another source of a potential performance improvement on tasks like a full kernel build, for example, where there are likely to be few other runnable tasks in the system.

\subsection{FLASH Size}
\label{subsec:flash_size}
In order to tackle the question of how does FLASH stack up to other embedded CPU accelerators (MMU, memory controller) of similar functionality, we also need to know the size (area and gate usage) of these components, as well as FLASH when synthesized for a specific technology node. Unfortunately, the information of these sizes is not made publically available, and would likely require us to get in contact with engineers at various Intel, AMD, or ARM.

\subsection{SMP Support}
The current implementation of FLASH has no knowledge of multiple processors. This is problematic bacause only one task can be running at a time from the point of view of FLASH. However, enabling SMP support in FLASH is not as simple as informing FLASH of the number of CPUs in a register, the hardware also needs to be able to make scheduling decisions for each of the processors efficiently. Naturally, this increases the logic usage for the scheduling kernel, but ideally the number of tasks supported in memory need not be increased, thus keeping the memory usage, and potentially area usage, still low and competitive with other CPU units.

\section{Conclusion}
\label{sec:conclusion}
% \lipsum[1]
In this paper, we present FLASH, a hardware scheduler accelerator designed to replace an operating system scheduler. FLASH performs scheduling decisions in hardware, thus freeing up the CPU and related structures to focus on computation. FLASH only interrupts the CPU when it has made a new decision, and only if that decision differs from the currently running task, thus reducing unnecessary interruptions. We will show the viability of FLASH hardware as an accelerator akin to the MMU by considering its power requirements and area usage.

\nocite{*}
{
	\bibliographystyle{abbrv}
	\bibliography{ref}
}

\end{document}
